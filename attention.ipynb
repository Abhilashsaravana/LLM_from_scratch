{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba627e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4da640",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "442b74b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[\"the\",\"sun\",\"rises\",\"in\",\"the\",\"east\"]\n",
    "model=Word2Vec([words],min_count=1,vector_size=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6c7e5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.15122044  0.21846838 -0.16200535]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv['sun'])  # Example to get the vector for the word 'sun'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9db98943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0179,  0.0079,  0.1701],\n",
      "        [-0.1512,  0.2185, -0.1620],\n",
      "        [-0.1254,  0.2460, -0.0511],\n",
      "        [ 0.2153,  0.2991, -0.1672],\n",
      "        [-0.0179,  0.0079,  0.1701],\n",
      "        [ 0.3003, -0.3101, -0.2372]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor(model.wv[words])  # Convert words to their corresponding vectors\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c41aa979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n",
      "tensor([-0.0231,  0.0968,  0.0810,  0.0599, -0.0231, -0.0747])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]  # 'sun'\n",
    "print(inputs.shape)\n",
    "scores = torch.empty(len(words))\n",
    "for i,x in enumerate(inputs):\n",
    "    scores[i] = torch.dot(query, x)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fba98b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1982,  0.8297,  0.6940,  0.5130, -0.1982, -0.6403])\n",
      "tensor([0.1594, 0.1797, 0.1769, 0.1732, 0.1594, 0.1514])\n"
     ]
    }
   ],
   "source": [
    "weights = scores/torch.sum(scores)\n",
    "print(weights)\n",
    "\n",
    "weights_softmax = torch.softmax(scores, dim=0)\n",
    "print(weights_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ab17524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0277,  0.0902, -0.0488])\n"
     ]
    }
   ],
   "source": [
    "context_vec2 = torch.zeros(3)\n",
    "for i, w in enumerate(weights_softmax):\n",
    "    context_vec2 += w * inputs[i]\n",
    "print(context_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d36d120f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0293, -0.0231, -0.0045, -0.0299,  0.0293, -0.0482],\n",
      "        [-0.0231,  0.0968,  0.0810,  0.0599, -0.0231, -0.0747],\n",
      "        [-0.0045,  0.0810,  0.0789,  0.0551, -0.0045, -0.1018],\n",
      "        [-0.0299,  0.0599,  0.0551,  0.1638, -0.0299,  0.0116],\n",
      "        [ 0.0293, -0.0231, -0.0045, -0.0299,  0.0293, -0.0482],\n",
      "        [-0.0482, -0.0747, -0.1018,  0.0116, -0.0482,  0.2426]])\n"
     ]
    }
   ],
   "source": [
    "attention_scores = torch.matmul(inputs, inputs.T)\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac5ca0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1729, 0.1641, 0.1672, 0.1630, 0.1729, 0.1600],\n",
      "        [0.1594, 0.1797, 0.1769, 0.1732, 0.1594, 0.1514],\n",
      "        [0.1627, 0.1773, 0.1769, 0.1727, 0.1627, 0.1476],\n",
      "        [0.1553, 0.1699, 0.1691, 0.1885, 0.1553, 0.1619],\n",
      "        [0.1729, 0.1641, 0.1672, 0.1630, 0.1729, 0.1600],\n",
      "        [0.1582, 0.1541, 0.1499, 0.1680, 0.1582, 0.2116]])\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0066da67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sums to 1: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "print(\"Sums to 1:\", attention_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49ccd8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0312,  0.0788, -0.0415],\n",
      "        [ 0.0277,  0.0902, -0.0488],\n",
      "        [ 0.0267,  0.0907, -0.0463],\n",
      "        [ 0.0367,  0.0873, -0.0532],\n",
      "        [ 0.0312,  0.0788, -0.0415],\n",
      "        [ 0.0519,  0.0577, -0.0571]])\n"
     ]
    }
   ],
   "source": [
    "contexts_vecs = torch.matmul(attention_weights, inputs)\n",
    "print(contexts_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125b83ca",
   "metadata": {},
   "source": [
    "Simple self attention mechanism with trainable weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f957543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0179,  0.0079,  0.1701],\n",
      "        [-0.1512,  0.2185, -0.1620],\n",
      "        [-0.1254,  0.2460, -0.0511],\n",
      "        [ 0.2153,  0.2991, -0.1672],\n",
      "        [-0.0179,  0.0079,  0.1701],\n",
      "        [ 0.3003, -0.3101, -0.2372]])\n",
      "Parameter containing:\n",
      "tensor([[0.1643, 0.8152],\n",
      "        [1.6379, 0.1752],\n",
      "        [1.3511, 0.3116]], requires_grad=True) Parameter containing:\n",
      "tensor([[ 0.3213,  0.2712],\n",
      "        [-2.0634, -0.5716],\n",
      "        [-0.6952,  0.0565]], requires_grad=True) Parameter containing:\n",
      "tensor([[-2.7896,  0.2080],\n",
      "        [-0.7742, -0.8000],\n",
      "        [ 0.3869,  1.2813]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(inputs)\n",
    "w_query=torch.nn.Parameter(torch.randn(inputs.shape[1],2),requires_grad=True)  \n",
    "w_key=torch.nn.Parameter(torch.randn(inputs.shape[1],2),requires_grad=True)  \n",
    "w_value=torch.nn.Parameter(torch.randn(inputs.shape[1],2),requires_grad=True)\n",
    "print(w_key,w_value,w_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46331260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1096,  0.2079],\n",
      "        [ 0.1900, -0.4138],\n",
      "        [ 0.1397, -0.2884],\n",
      "        [-0.8968, -0.4087],\n",
      "        [ 0.1096,  0.2079],\n",
      "        [-0.6894,  0.0066]], grad_fn=<MmBackward0>) tensor([[ 0.2398,  0.0398],\n",
      "        [ 0.1141, -0.1355],\n",
      "        [ 0.3133, -0.0751],\n",
      "        [ 0.2994,  0.1758],\n",
      "        [ 0.2398,  0.0398],\n",
      "        [-0.7791,  0.1166]], grad_fn=<MmBackward0>) tensor([[-1.4027e-01,  2.5859e-04],\n",
      "        [-3.8674e-01, -1.7504e-01],\n",
      "        [-5.1239e-01, -1.7754e-01],\n",
      "        [-4.3176e-01, -1.2203e-01],\n",
      "        [-1.4027e-01,  2.5859e-04],\n",
      "        [ 9.0125e-01,  2.4530e-01]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "query2 = torch.matmul(inputs, w_query)\n",
    "keys2 = torch.matmul(inputs, w_key)\n",
    "values2 = torch.matmul(inputs, w_value)\n",
    "print(query2,keys2,values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c944992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0346, -0.0157,  0.0187,  0.0694,  0.0346, -0.0611],\n",
      "        [ 0.0291,  0.0778,  0.0906, -0.0159,  0.0291, -0.1963],\n",
      "        [ 0.0220,  0.0550,  0.0654, -0.0089,  0.0220, -0.1425],\n",
      "        [-0.2313, -0.0470, -0.2503, -0.3404, -0.2313,  0.6511],\n",
      "        [ 0.0346, -0.0157,  0.0187,  0.0694,  0.0346, -0.0611],\n",
      "        [-0.1651, -0.0796, -0.2165, -0.2052, -0.1651,  0.5379]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "tensor([[0.1691, 0.1632, 0.1672, 0.1733, 0.1691, 0.1580],\n",
      "        [0.1695, 0.1754, 0.1770, 0.1642, 0.1695, 0.1445],\n",
      "        [0.1688, 0.1728, 0.1741, 0.1652, 0.1688, 0.1503],\n",
      "        [0.1445, 0.1647, 0.1426, 0.1338, 0.1445, 0.2698],\n",
      "        [0.1691, 0.1632, 0.1672, 0.1733, 0.1691, 0.1580],\n",
      "        [0.1505, 0.1599, 0.1452, 0.1463, 0.1505, 0.2475]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attention_scores2 = torch.matmul(query2, keys2.T)\n",
    "print(attention_scores2)\n",
    "attention_scores2norm = torch.softmax(attention_scores2/(2**0.5), dim=-1)\n",
    "print(attention_scores2norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bdd303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class selfattention(torch.nn.Module):\n",
    "    def __init__(self, input_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.w_query = torch.nn.Parameter(torch.randn(input_dim, out_dim), requires_grad=True)\n",
    "        self.w_key = torch.nn.Parameter(torch.randn(input_dim, out_dim), requires_grad=True)\n",
    "        self.w_value = torch.nn.Parameter(torch.randn(input_dim, out_dim), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = torch.matmul(x, self.w_query)\n",
    "        keys = torch.matmul(x, self.w_key)\n",
    "        values = torch.matmul(x, self.w_value)\n",
    "\n",
    "        attention_scores = torch.matmul(queries, keys.T) \n",
    "        attention_weights = torch.softmax(attention_scores/(keys.shape[1]**0.5), dim=-1)\n",
    "\n",
    "        output = torch.matmul(attention_weights, values)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a5bb8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1070, -0.0117],\n",
      "        [-0.0941, -0.0277],\n",
      "        [-0.0958, -0.0253],\n",
      "        [-0.0983, -0.0206],\n",
      "        [-0.1070, -0.0117],\n",
      "        [-0.1123, -0.0052]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test_attention = selfattention(input_dim=3, out_dim=2)\n",
    "output = test_attention(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3e4d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class selfattentionv2(torch.nn.Module):\n",
    "    def __init__(self, input_dim, out_dim,qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.w_query = torch.nn.Linear(input_dim, out_dim,bias=qkv_bias)\n",
    "        self.w_key = torch.nn.Linear(input_dim, out_dim,bias=qkv_bias)\n",
    "        self.w_value = torch.nn.Linear(input_dim, out_dim,bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.w_query(x)\n",
    "        keys = self.w_key(x)\n",
    "        values = self.w_value(x)\n",
    "\n",
    "        attention_scores = torch.matmul(queries, keys.T) \n",
    "        attention_weights = torch.softmax(attention_scores/(keys.shape[1]**0.5), dim=-1)\n",
    "\n",
    "        output = torch.matmul(attention_weights, values)\n",
    "        return output,attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba162dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1670, 0.1671, 0.1671, 0.1663, 0.1670, 0.1655],\n",
      "        [0.1653, 0.1672, 0.1663, 0.1665, 0.1653, 0.1693],\n",
      "        [0.1655, 0.1674, 0.1665, 0.1664, 0.1655, 0.1688],\n",
      "        [0.1653, 0.1662, 0.1654, 0.1672, 0.1653, 0.1706],\n",
      "        [0.1670, 0.1671, 0.1671, 0.1663, 0.1670, 0.1655],\n",
      "        [0.1677, 0.1647, 0.1658, 0.1678, 0.1677, 0.1663]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test2 = selfattentionv2(input_dim=3, out_dim=2)\n",
    "output2,weights = test2.forward(inputs)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04acdb67",
   "metadata": {},
   "source": [
    "<b>Causal Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c57ace1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones(weights.shape))\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c412bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1670, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1653, 0.1672, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1655, 0.1674, 0.1665, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1653, 0.1662, 0.1654, 0.1672, 0.0000, 0.0000],\n",
      "        [0.1670, 0.1671, 0.1671, 0.1663, 0.1670, 0.0000],\n",
      "        [0.1677, 0.1647, 0.1658, 0.1678, 0.1677, 0.1663]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_weights = weights * mask\n",
    "print(masked_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d05f8f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4971, 0.5029, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3313, 0.3353, 0.3334, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2488, 0.2503, 0.2491, 0.2518, 0.0000, 0.0000],\n",
      "        [0.2001, 0.2002, 0.2003, 0.1994, 0.2001, 0.0000],\n",
      "        [0.1677, 0.1647, 0.1658, 0.1678, 0.1677, 0.1663]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_weights.sum(dim=-1, keepdim=True)\n",
    "normalized_masked_weights = masked_weights / row_sums \n",
    "print(normalized_masked_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53314e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1670,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1653, 0.1672,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1655, 0.1674, 0.1665,   -inf,   -inf,   -inf],\n",
      "        [0.1653, 0.1662, 0.1654, 0.1672,   -inf,   -inf],\n",
      "        [0.1670, 0.1671, 0.1671, 0.1663, 0.1670,   -inf],\n",
      "        [0.1677, 0.1647, 0.1658, 0.1678, 0.1677, 0.1663]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(weights.shape), diagonal=1)\n",
    "masked=weights.masked_fill(mask.bool(), float('-inf'))\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1dbba17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4997, 0.5003, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3331, 0.3336, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2499, 0.2500, 0.2499, 0.2502, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.1999, 0.2000, 0.0000],\n",
      "        [0.1668, 0.1664, 0.1666, 0.1668, 0.1668, 0.1666]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "weights = torch.softmax(masked/keys2.shape[-1]**0.5, dim=1)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8d5889",
   "metadata": {},
   "source": [
    "<b>Attention weights with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afb4aa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0007, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6667, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4997, 0.5001, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3329, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(p=0.5)\n",
    "dropped_weights = dropout(weights)\n",
    "print(dropped_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6398f6c4",
   "metadata": {},
   "source": [
    "<b>Causal Attention Mechanism Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b451b355",
   "metadata": {},
   "outputs": [],
   "source": [
    "class causalattention(torch.nn.Module):\n",
    "    def __init__(self, input_dim, out_dim,context_len,dropout,qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.w_query = torch.nn.Linear(input_dim, out_dim,bias=qkv_bias)\n",
    "        self.w_key = torch.nn.Linear(input_dim, out_dim,bias=qkv_bias)\n",
    "        self.w_value = torch.nn.Linear(input_dim, out_dim,bias=qkv_bias)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_len, context_len), diagonal=1))\n",
    "    def forward(self, x):\n",
    "        b,num_tokens,input_dim = x.shape\n",
    "        queries = self.w_query(x)\n",
    "        keys = self.w_key(x)\n",
    "        values = self.w_value(x)\n",
    "\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(1,2))\n",
    "        attention_scores=attention_scores.masked_fill(self.mask.bool()[:num_tokens,:num_tokens], float('-inf')) \n",
    "        attention_weights = torch.softmax(attention_scores/(keys.shape[1]**0.5), dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        output = torch.matmul(attention_weights, values)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83ee3011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "tensor([[[ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0055,  0.2110],\n",
      "         [-0.0492,  0.1841],\n",
      "         [-0.0018,  0.0584],\n",
      "         [-0.0016,  0.0483]],\n",
      "\n",
      "        [[-0.0294, -0.0246],\n",
      "         [ 0.0254,  0.1736],\n",
      "         [ 0.0168,  0.1152],\n",
      "         [ 0.0125,  0.0858],\n",
      "         [-0.0437,  0.0797],\n",
      "         [-0.0355, -0.0025]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs,inputs),dim=0)  # Create a batch of 2 identical sequences\n",
    "print(batch.shape)\n",
    "test3 = causalattention(input_dim=3, out_dim=2,context_len=6,dropout=0.5)\n",
    "contexts_vecs = test3.forward(batch)\n",
    "print(contexts_vecs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f13f15",
   "metadata": {},
   "source": [
    "<b> Multi Head attention mechanism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4124f720",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiheadattentionv1(torch.nn.Module):\n",
    "    def __init__(self, input_dim, out_dim, num_heads, context_len, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_heads = torch.nn.ModuleList([\n",
    "            causalattention(input_dim, out_dim, context_len, dropout, qkv_bias) \n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.attention_heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33734ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0155, -0.0989, -0.0257,  0.0369],\n",
      "         [-0.0426, -0.0455, -0.0035, -0.0393],\n",
      "         [ 0.0407, -0.0665, -0.0009, -0.0094],\n",
      "         [ 0.0250, -0.1343, -0.0020, -0.0236],\n",
      "         [ 0.0415, -0.0551,  0.0331, -0.0256]],\n",
      "\n",
      "        [[ 0.1580, -0.0613,  0.0000,  0.0000],\n",
      "         [-0.0637, -0.0681, -0.0257,  0.0369],\n",
      "         [ 0.0119, -0.1339, -0.0183,  0.0120],\n",
      "         [-0.0095, -0.1013, -0.0035, -0.0390],\n",
      "         [ 0.0250, -0.1343, -0.0102,  0.0147],\n",
      "         [ 0.0049, -0.0333,  0.0029,  0.0358]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test4 = Multiheadattentionv1(input_dim=3, out_dim=2, num_heads=2, context_len=6, dropout=0.5)\n",
    "contexts_vecs_multihead = test4.forward(batch)\n",
    "print(contexts_vecs_multihead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeac7af",
   "metadata": {},
   "source": [
    "<b>Multi head attention with weight splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db2b4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiheadattentionv2(torch.nn.Module):\n",
    "    def __init__(self, input_dim, out_dim, num_heads, context_len, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert out_dim % num_heads == 0, \"out_dim must be divisible by num_heads\"\n",
    "        self.out_dim = out_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = out_dim // num_heads\n",
    "        self.w_query = torch.nn.Linear(input_dim, out_dim, bias=qkv_bias)\n",
    "        self.w_key = torch.nn.Linear(input_dim, out_dim, bias=qkv_bias)\n",
    "        self.w_value = torch.nn.Linear(input_dim, out_dim, bias=qkv_bias)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_len, context_len), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, input_dim = x.shape\n",
    "        \n",
    "        # Split and reshape for multi-head attention\n",
    "        queries = self.w_query(x).view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        keys = self.w_key(x).view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        values = self.w_value(x).view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        \n",
    "        attention_scores = torch.matmul(queries, keys.transpose(2, 3))\n",
    "        attention_scores = attention_scores.masked_fill(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attention_weights = torch.softmax(attention_scores/keys.shape[-1]**0.5, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        output = torch.matmul(attention_weights, values).transpose(1, 2).contiguous().view(b, num_tokens, self.out_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10217611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0089,  0.0039,  0.0851,  0.1502, -0.1550, -0.1186],\n",
      "        [ 0.1058, -0.0568, -0.0158,  0.0961, -0.1254, -0.0656],\n",
      "        [-0.1381, -0.1575,  0.1219,  0.0845,  0.1126,  0.0127],\n",
      "        [-0.0756,  0.1092, -0.0810, -0.0303,  0.0479,  0.0165],\n",
      "        [-0.0089,  0.0039,  0.0851,  0.1502, -0.1550, -0.1186],\n",
      "        [ 0.1076,  0.1495, -0.0836, -0.0627,  0.1230, -0.0256]])\n",
      "torch.Size([2, 6, 6])\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0037, -0.0019,  0.0136, -0.0799,  0.0117, -0.0673],\n",
      "         [-0.0152,  0.0084,  0.0005, -0.0534,  0.0078, -0.0450],\n",
      "         [-0.0286, -0.0125, -0.0564, -0.0629,  0.0240, -0.0547],\n",
      "         [-0.0134, -0.0146, -0.0363, -0.0235,  0.0160,  0.0019],\n",
      "         [-0.0097, -0.0127, -0.0251, -0.0686,  0.0199, -0.0589]],\n",
      "\n",
      "        [[ 0.0074, -0.0038,  0.0272,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000, -0.0799,  0.0117, -0.0673],\n",
      "         [-0.0152,  0.0084,  0.0005,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0036, -0.0506, -0.0404],\n",
      "         [-0.0109, -0.0167, -0.0351, -0.0207, -0.0243, -0.0304],\n",
      "         [-0.0010,  0.0139,  0.0250, -0.0856,  0.0307, -0.0236]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "words=[\"the\",\"sun\",\"rises\",\"in\",\"the\",\"east\"]\n",
    "model=Word2Vec([words],min_count=1,vector_size=6)\n",
    "inputs = torch.tensor(model.wv[words])  # Convert words to their corresponding vectors\n",
    "print(inputs)\n",
    "batch = torch.stack((inputs,inputs),dim=0)  # Create a batch of 2 identical sequences\n",
    "print(batch.shape)\n",
    "test4 = Multiheadattentionv2(input_dim=6, out_dim=6, num_heads=2, context_len=6, dropout=0.5)\n",
    "contexts_vecs_multihead = test4.forward(batch)\n",
    "print(contexts_vecs_multihead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5527ddc",
   "metadata": {},
   "source": [
    "<b>Implementing Dummy GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "094881fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M={\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_len\": 1024,\n",
    "    \"n_embd\": 768,\n",
    "    \"n_layer\": 12,\n",
    "    \"n_head\": 12,\n",
    "    \"dropout\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "461faeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class DummyGPTModel(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embedding = torch.nn.Embedding(config['vocab_size'], config['n_embd'])\n",
    "        self.position_embedding = torch.nn.Embedding(config['context_len'], config['n_embd'])\n",
    "        self.dropout = torch.nn.Dropout(config['dropout'])\n",
    "        \n",
    "        #Placeholder for the actual transformer blocks\n",
    "        self.trf = torch.nn.Sequential(\n",
    "            *[DummyTransformer(config) for _ in range(config['n_layer'])])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.final_norm = DummyLayerNorm(config['n_embd'])\n",
    "        self.head = torch.nn.Linear(config['n_embd'], config['vocab_size'], bias=False)\n",
    "        \n",
    "\n",
    "    def forward(self, idx):\n",
    "        b, t = idx.shape\n",
    "        token_embeddings = self.token_embedding(idx)\n",
    "        position_embeddings = self.position_embedding(torch.arange(t, device=idx.device))\n",
    "        \n",
    "        x = token_embeddings + position_embeddings\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.trf(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "class DummyTransformer(torch.nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "    def forward(self,x):\n",
    "        return x\n",
    "class DummyLayerNorm(torch.nn.Module):\n",
    "    def __init__(self,shape,eps=1e-5):\n",
    "        super().__init__()\n",
    "    def forward(self,x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c49ad57",
   "metadata": {},
   "source": [
    "<b> Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f558296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  464,  4252, 16736,   287,   262],\n",
      "        [  464,  4252,  5621,   287,   262]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch=[]\n",
    "txt1 = \"The sun rises in the\"\n",
    "txt2 = \"The sun sets in the\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbce10a",
   "metadata": {},
   "source": [
    "<b>Instance of DummyGPTModel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50a92944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 50257])\n",
      "tensor([[[ 0.5209,  0.5520, -0.1626,  ..., -0.6756,  0.4891,  1.0015],\n",
      "         [ 0.2441,  1.2358,  0.7340,  ...,  0.8618,  0.6503,  0.6837],\n",
      "         [-0.1495,  0.3894, -0.2040,  ...,  0.3658, -0.6975,  0.0046],\n",
      "         [-0.4361,  2.2998, -0.8146,  ...,  1.0815, -0.0610,  0.4576],\n",
      "         [-0.6581, -0.4606,  0.6273,  ...,  0.6359, -1.1219, -0.1483]],\n",
      "\n",
      "        [[ 0.6502,  0.6544, -0.4624,  ..., -0.6154,  0.2181,  1.1435],\n",
      "         [ 0.4852,  1.2206, -0.0328,  ...,  0.7643,  0.6751,  0.7432],\n",
      "         [ 0.4855,  0.2990, -0.0415,  ..., -0.3032,  0.9354,  0.3412],\n",
      "         [-0.1921,  1.6800, -0.7408,  ...,  0.7503,  0.0276,  0.4491],\n",
      "         [-0.4396, -0.8658,  0.2206,  ...,  0.7252, -1.3121,  0.0530]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e93ddef",
   "metadata": {},
   "source": [
    "<b> Layer Normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5ce29af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(torch.nn.Module):\n",
    "    def __init__(self, n_embd, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(n_embd))\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(n_embd))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        variance = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        x_normalized = (x - mean) / torch.sqrt(variance + self.eps)\n",
    "        return self.gamma * x_normalized + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "657cb643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.1467e-08],\n",
      "        [6.9539e-08]], grad_fn=<MeanBackward1>)\n",
      "tensor([[1.1999],\n",
      "        [1.1999]], grad_fn=<VarBackward0>)\n",
      "tensor([[ 0.3324,  0.8349, -0.2272,  0.0529, -2.0204,  1.0276],\n",
      "        [-1.4542, -0.9964,  1.2767,  0.3657,  1.0374, -0.2291]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch = torch.randn(2, 6)\n",
    "ln = LayerNorm(n_embd=6)\n",
    "output = ln.forward(batch)\n",
    "mean = output.mean(dim=-1,keepdim=True)\n",
    "var = output.var(dim=-1,keepdim=True)\n",
    "print(mean)\n",
    "print(var)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312657d3",
   "metadata": {},
   "source": [
    "<b> GELU Activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df3b1807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class GELU(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi)) * (x + 0.044715 * (x ** 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19201091",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(cfg['n_embd'], cfg['n_embd']*4),\n",
    "            GELU(),\n",
    "            torch.nn.Linear(cfg['n_embd']*4, cfg['n_embd']),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4435666e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "batch = torch.randn(2, 6, 768)\n",
    "output = ffn.forward(batch)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd581f37",
   "metadata": {},
   "source": [
    "<b>Transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "609c70b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_block(torch.nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(cfg['n_embd'])\n",
    "        self.attn = Multiheadattentionv2(cfg['n_embd'], cfg['n_embd'], cfg['n_head'], cfg['context_len'], cfg['dropout'], cfg['qkv_bias'])\n",
    "        self.ln2 = LayerNorm(cfg['n_embd'])\n",
    "        self.ffn = FeedForward(cfg)\n",
    "        self.dropout = torch.nn.Dropout(cfg['dropout'])\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.ln1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "        \n",
    "        shortcut = x\n",
    "        x = self.ln2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "169e991e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 768])\n",
      "torch.Size([2, 6, 768])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(2, 6, 768)\n",
    "block = transformer_block(GPT_CONFIG_124M)\n",
    "output = block.forward(input)\n",
    "print(input.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de947433",
   "metadata": {},
   "source": [
    "<b> Implementing GPT from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6eac9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M={\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_len\": 1024,\n",
    "    \"n_embd\": 768,\n",
    "    \"n_layer\": 12,\n",
    "    \"n_head\": 12,\n",
    "    \"dropout\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "15369aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embedding = torch.nn.Embedding(config['vocab_size'], config['n_embd'])\n",
    "        self.position_embedding = torch.nn.Embedding(config['context_len'], config['n_embd'])\n",
    "        self.dropout = torch.nn.Dropout(config['dropout'])\n",
    "        \n",
    "        self.trf = torch.nn.Sequential(\n",
    "            *[transformer_block(config) for _ in range(config['n_layer'])])\n",
    "\n",
    "        self.final_norm = LayerNorm(config['n_embd'])\n",
    "        self.head = torch.nn.Linear(config['n_embd'], config['vocab_size'], bias=False)\n",
    "        \n",
    "\n",
    "    def forward(self, idx):\n",
    "        b, t = idx.shape\n",
    "        token_embeddings = self.token_embedding(idx)\n",
    "        position_embeddings = self.position_embedding(torch.arange(t, device=idx.device))\n",
    "        \n",
    "        x = token_embeddings + position_embeddings\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.trf(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f0a6c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "tensor([[  464,  4252, 16736,   287],\n",
      "        [  464,  4252,  5621,   287]])\n",
      "torch.Size([2, 4, 50257])\n",
      "tensor([[[ 6.1162e-01,  2.5310e-01,  5.6558e-01,  ...,  8.1469e-02,\n",
      "          -3.8495e-01,  4.2311e-01],\n",
      "         [-5.5372e-02,  2.4678e-02,  5.1740e-01,  ...,  2.6348e-01,\n",
      "          -2.5397e-01,  1.1299e+00],\n",
      "         [-3.9154e-01,  1.0788e+00,  3.9125e-01,  ..., -3.8340e-01,\n",
      "           2.1165e-01,  1.7440e-01],\n",
      "         [ 4.4995e-02,  2.3623e-01,  2.8161e-01,  ...,  4.3975e-01,\n",
      "          -3.6521e-01,  7.5584e-02]],\n",
      "\n",
      "        [[ 9.7846e-01,  3.7193e-01, -4.8595e-02,  ...,  3.2563e-01,\n",
      "           1.8810e-01,  1.9463e-02],\n",
      "         [ 7.1756e-01,  1.7630e-01, -7.2398e-02,  ...,  6.4695e-01,\n",
      "           2.2511e-01,  5.7401e-01],\n",
      "         [ 4.6550e-01,  1.2275e-01, -2.2191e-01,  ...,  9.9458e-04,\n",
      "           3.0963e-01, -1.9097e-01],\n",
      "         [ 9.2756e-01,  3.8084e-01, -2.3313e-02,  ...,  1.1447e+00,\n",
      "           2.7680e-02,  8.2508e-01]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch=[]\n",
    "txt1 = \"The sun rises in\"\n",
    "txt2 = \"The sun sets in\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "print(batch.shape)\n",
    "print(batch)\n",
    "output = model(batch)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4a43533b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 155,922,432\n"
     ]
    }
   ],
   "source": [
    "total_param = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_param:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7033bf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters without the head: 117,325,056\n"
     ]
    }
   ],
   "source": [
    "total_param_2=total_param-sum(p.numel() for p in model.head.parameters())\n",
    "print(f\"Total parameters without the head: {total_param_2:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5052e622",
   "metadata": {},
   "source": [
    "<b> Generating text from output tokens:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab6fb80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output_txt(model,input, max_new_tokens,context_len):\n",
    "    for i in range(max_new_tokens):\n",
    "        input_cond = input[:,-context_len:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_cond)\n",
    "        logits = logits[:,-1,:]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.argmax(probs,dim=-1,keepdim=True)\n",
    "        input = torch.cat((input, next_token), dim=1)\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "836d344d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  464,  4252, 16736,   287]])\n"
     ]
    }
   ],
   "source": [
    "input = \"The sun rises in\"\n",
    "encoded_input = torch.tensor(tokenizer.encode(input)).unsqueeze(0) \n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "344e356f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  464,  4252, 16736,   287, 46408, 35321, 33974, 29725,  3993,  5576]])\n",
      "The sun rises in breaker Marina MJeware sleep uns\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "output_tokens = generate_output_txt(model, encoded_input, max_new_tokens=6, context_len=GPT_CONFIG_124M['context_len'])\n",
    "print(output_tokens)\n",
    "print(tokenizer.decode(output_tokens[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4ecab6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun rises in breaker Marina MJeware sleep uns\n"
     ]
    }
   ],
   "source": [
    "def text_to_tokens(text,tokenizer):\n",
    "    return torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
    "def tokens_to_text(tokens,tokenizer):\n",
    "    return tokenizer.decode((tokens.squeeze(0)).tolist())\n",
    "\n",
    "ex = \"The sun rises in\"\n",
    "toeken_ids=generate_output_txt(model,text_to_tokens(ex,tokenizer), max_new_tokens=6,context_len=GPT_CONFIG_124M['context_len'])\n",
    "print(tokens_to_text(toeken_ids,tokenizer))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5ecc08e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[16833,  3626,  6100],\n",
      "        [   40,  1107,   588]])\n",
      "tensor([[ 3626,  6100,   345],\n",
      "        [ 1107,   588, 11311]])\n"
     ]
    }
   ],
   "source": [
    "inputs = text_to_tokens(\"every effort moves\",tokenizer)\n",
    "inputs = torch.cat([inputs, text_to_tokens(\"I really like\",tokenizer)])\n",
    "print(inputs)\n",
    "target = torch.tensor([[3626,6100,345],\n",
    "                       [1107,588,11311]])\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0b59df84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11821, 38580, 39436],\n",
      "        [10352, 38988, 13436]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probabs = torch.softmax(logits, dim=-1)\n",
    "output = torch.argmax(probabs,dim=-1)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1b5dbca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:  effort moves you\n",
      "Output:  spokesperson Gallagher hr\n"
     ]
    }
   ],
   "source": [
    "print(\"Target:\",tokens_to_text(target[0],tokenizer))\n",
    "print(\"Output:\",tokens_to_text(output[0],tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4372b000",
   "metadata": {},
   "source": [
    "<b> Cross Entropy Loss :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "69596632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5349e-05, 3.3739e-05, 3.2623e-05])\n",
      "tensor([2.4294e-05, 2.8897e-05, 3.2795e-05])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0 \n",
    "target_1 = probabs[text_idx,[0,1,2],target[text_idx]]\n",
    "print(target_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_2 = probabs[text_idx,[0,1,2],target[text_idx]]\n",
    "print(target_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "57840d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-11.0845, -10.2969, -10.3305, -10.6253, -10.4518, -10.3252])\n"
     ]
    }
   ],
   "source": [
    "#Log of all token probability\n",
    "log_probas  = torch.log(torch.cat((target_1,target_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e4993733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.5190)\n"
     ]
    }
   ],
   "source": [
    "#Calculate average probability\n",
    "avg_log_prob = torch.mean(log_probas)\n",
    "print(avg_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0948ec01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.5190)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_prob = -avg_log_prob\n",
    "print(neg_avg_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8466c92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 50257]) torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "#Using PyTorch Cross Entropy Loss\n",
    "logits_flat = logits.flatten(0,1)\n",
    "target = target.flatten()\n",
    "print(logits_flat.shape,target.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b831eaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.5190)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, target)\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep)",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
