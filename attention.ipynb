{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba627e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4da640",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "442b74b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[\"the\",\"sun\",\"rises\",\"in\",\"the\",\"east\"]\n",
    "model=Word2Vec([words],min_count=1,vector_size=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6c7e5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.15122044  0.21846838 -0.16200535]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv['sun'])  # Example to get the vector for the word 'sun'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9db98943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0179,  0.0079,  0.1701],\n",
      "        [-0.1512,  0.2185, -0.1620],\n",
      "        [-0.1254,  0.2460, -0.0511],\n",
      "        [ 0.2153,  0.2991, -0.1672],\n",
      "        [-0.0179,  0.0079,  0.1701],\n",
      "        [ 0.3003, -0.3101, -0.2372]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor(model.wv[words])  # Convert words to their corresponding vectors\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c41aa979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n",
      "tensor([-0.0231,  0.0968,  0.0810,  0.0599, -0.0231, -0.0747])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]  # 'sun'\n",
    "print(inputs.shape)\n",
    "scores = torch.empty(len(words))\n",
    "for i,x in enumerate(inputs):\n",
    "    scores[i] = torch.dot(query, x)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fba98b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1982,  0.8297,  0.6940,  0.5130, -0.1982, -0.6403])\n",
      "tensor([0.1594, 0.1797, 0.1769, 0.1732, 0.1594, 0.1514])\n"
     ]
    }
   ],
   "source": [
    "weights = scores/torch.sum(scores)\n",
    "print(weights)\n",
    "\n",
    "weights_softmax = torch.softmax(scores, dim=0)\n",
    "print(weights_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ab17524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0277,  0.0902, -0.0488])\n"
     ]
    }
   ],
   "source": [
    "context_vec2 = torch.zeros(3)\n",
    "for i, w in enumerate(weights_softmax):\n",
    "    context_vec2 += w * inputs[i]\n",
    "print(context_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d36d120f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0293, -0.0231, -0.0045, -0.0299,  0.0293, -0.0482],\n",
      "        [-0.0231,  0.0968,  0.0810,  0.0599, -0.0231, -0.0747],\n",
      "        [-0.0045,  0.0810,  0.0789,  0.0551, -0.0045, -0.1018],\n",
      "        [-0.0299,  0.0599,  0.0551,  0.1638, -0.0299,  0.0116],\n",
      "        [ 0.0293, -0.0231, -0.0045, -0.0299,  0.0293, -0.0482],\n",
      "        [-0.0482, -0.0747, -0.1018,  0.0116, -0.0482,  0.2426]])\n"
     ]
    }
   ],
   "source": [
    "attention_scores = torch.matmul(inputs, inputs.T)\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac5ca0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1729, 0.1641, 0.1672, 0.1630, 0.1729, 0.1600],\n",
      "        [0.1594, 0.1797, 0.1769, 0.1732, 0.1594, 0.1514],\n",
      "        [0.1627, 0.1773, 0.1769, 0.1727, 0.1627, 0.1476],\n",
      "        [0.1553, 0.1699, 0.1691, 0.1885, 0.1553, 0.1619],\n",
      "        [0.1729, 0.1641, 0.1672, 0.1630, 0.1729, 0.1600],\n",
      "        [0.1582, 0.1541, 0.1499, 0.1680, 0.1582, 0.2116]])\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0066da67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sums to 1: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "print(\"Sums to 1:\", attention_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49ccd8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0312,  0.0788, -0.0415],\n",
      "        [ 0.0277,  0.0902, -0.0488],\n",
      "        [ 0.0267,  0.0907, -0.0463],\n",
      "        [ 0.0367,  0.0873, -0.0532],\n",
      "        [ 0.0312,  0.0788, -0.0415],\n",
      "        [ 0.0519,  0.0577, -0.0571]])\n"
     ]
    }
   ],
   "source": [
    "contexts_vecs = torch.matmul(attention_weights, inputs)\n",
    "print(contexts_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125b83ca",
   "metadata": {},
   "source": [
    "Simple self attention mechanism with trainable weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f957543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0179,  0.0079,  0.1701],\n",
      "        [-0.1512,  0.2185, -0.1620],\n",
      "        [-0.1254,  0.2460, -0.0511],\n",
      "        [ 0.2153,  0.2991, -0.1672],\n",
      "        [-0.0179,  0.0079,  0.1701],\n",
      "        [ 0.3003, -0.3101, -0.2372]])\n",
      "Parameter containing:\n",
      "tensor([[-0.3101, -3.1771],\n",
      "        [ 0.3294,  0.9397],\n",
      "        [-1.1407,  0.6060]], requires_grad=True) Parameter containing:\n",
      "tensor([[ 0.8094,  0.5445],\n",
      "        [-0.2459, -1.0030],\n",
      "        [ 0.6025,  0.8385]], requires_grad=True) Parameter containing:\n",
      "tensor([[ 0.5728, -0.3479],\n",
      "        [-0.0698,  1.0605],\n",
      "        [-0.1874,  0.3609]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(inputs)\n",
    "w_query=torch.nn.Parameter(torch.randn(inputs.shape[1],2),requires_grad=True)  \n",
    "w_key=torch.nn.Parameter(torch.randn(inputs.shape[1],2),requires_grad=True)  \n",
    "w_value=torch.nn.Parameter(torch.randn(inputs.shape[1],2),requires_grad=True)\n",
    "print(w_key,w_value,w_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46331260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0427,  0.0760],\n",
      "        [-0.0715,  0.2258],\n",
      "        [-0.0794,  0.2861],\n",
      "        [ 0.1338,  0.1820],\n",
      "        [-0.0427,  0.0760],\n",
      "        [ 0.2381, -0.5190]], grad_fn=<MmBackward0>) tensor([[-0.1859,  0.1673],\n",
      "        [ 0.3037,  0.5876],\n",
      "        [ 0.1783,  0.5988],\n",
      "        [ 0.2225, -0.5043],\n",
      "        [-0.1859,  0.1673],\n",
      "        [ 0.0753, -1.3893]], grad_fn=<MmBackward0>) tensor([[ 8.6086e-02,  1.2500e-01],\n",
      "        [-2.7372e-01, -4.3730e-01],\n",
      "        [-1.9282e-01, -3.5791e-01],\n",
      "        [-7.8226e-06, -3.2296e-01],\n",
      "        [ 8.6086e-02,  1.2500e-01],\n",
      "        [ 1.7639e-01,  2.7561e-01]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "query2 = torch.matmul(inputs, w_query)\n",
    "keys2 = torch.matmul(inputs, w_key)\n",
    "values2 = torch.matmul(inputs, w_value)\n",
    "print(query2,keys2,values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c944992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0206,  0.0317,  0.0379, -0.0478,  0.0206, -0.1088],\n",
      "        [ 0.0511,  0.1110,  0.1225, -0.1298,  0.0511, -0.3191],\n",
      "        [ 0.0626,  0.1440,  0.1571, -0.1619,  0.0626, -0.4034],\n",
      "        [ 0.0056,  0.1475,  0.1328, -0.0620,  0.0056, -0.2427],\n",
      "        [ 0.0206,  0.0317,  0.0379, -0.0478,  0.0206, -0.1088],\n",
      "        [-0.1311, -0.2326, -0.2683,  0.3147, -0.1311,  0.7389]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "tensor([[0.1699, 0.1712, 0.1720, 0.1619, 0.1699, 0.1551],\n",
      "        [0.1741, 0.1816, 0.1831, 0.1532, 0.1741, 0.1340],\n",
      "        [0.1754, 0.1858, 0.1875, 0.1497, 0.1754, 0.1262],\n",
      "        [0.1669, 0.1845, 0.1826, 0.1591, 0.1669, 0.1400],\n",
      "        [0.1699, 0.1712, 0.1720, 0.1619, 0.1699, 0.1551],\n",
      "        [0.1417, 0.1319, 0.1286, 0.1942, 0.1417, 0.2621]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attention_scores2 = torch.matmul(query2, keys2.T)\n",
    "print(attention_scores2)\n",
    "attention_scores2norm = torch.softmax(attention_scores2/(2**0.5), dim=-1)\n",
    "print(attention_scores2norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bdd303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class selfattention(torch.nn.Module):\n",
    "    def __init__(self, input_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.w_query = torch.nn.Parameter(torch.randn(input_dim, out_dim), requires_grad=True)\n",
    "        self.w_key = torch.nn.Parameter(torch.randn(input_dim, out_dim), requires_grad=True)\n",
    "        self.w_value = torch.nn.Parameter(torch.randn(input_dim, out_dim), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = torch.matmul(x, self.w_query)\n",
    "        keys = torch.matmul(x, self.w_key)\n",
    "        values = torch.matmul(x, self.w_value)\n",
    "\n",
    "        attention_scores = torch.matmul(queries, keys.T) \n",
    "        attention_weights = torch.softmax(attention_scores/(keys.shape[1]**0.5), dim=-1)\n",
    "\n",
    "        output = torch.matmul(attention_weights, values)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a5bb8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1960, -0.1268],\n",
      "        [-0.1277, -0.0922],\n",
      "        [-0.1442, -0.1000],\n",
      "        [-0.1721, -0.1161],\n",
      "        [-0.1960, -0.1268],\n",
      "        [-0.2002, -0.1329]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test_attention = selfattention(input_dim=3, out_dim=2)\n",
    "output = test_attention(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3e4d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class selfattentionv2(torch.nn.Module):\n",
    "    def __init__(self, input_dim, out_dim,qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.w_query = torch.nn.Linear(input_dim, out_dim,bias=qkv_bias)\n",
    "        self.w_key = torch.nn.Linear(input_dim, out_dim,bias=qkv_bias)\n",
    "        self.w_value = torch.nn.Linear(input_dim, out_dim,bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.w_query(x)\n",
    "        keys = self.w_key(x)\n",
    "        values = self.w_value(x)\n",
    "\n",
    "        attention_scores = torch.matmul(queries, keys.T) \n",
    "        attention_weights = torch.softmax(attention_scores/(keys.shape[1]**0.5), dim=-1)\n",
    "\n",
    "        output = torch.matmul(attention_weights, values)\n",
    "        return output,attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba162dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1670, 0.1672, 0.1675, 0.1668, 0.1670, 0.1645],\n",
      "        [0.1653, 0.1648, 0.1637, 0.1661, 0.1653, 0.1748],\n",
      "        [0.1655, 0.1652, 0.1642, 0.1662, 0.1655, 0.1733],\n",
      "        [0.1657, 0.1656, 0.1647, 0.1662, 0.1657, 0.1721],\n",
      "        [0.1670, 0.1672, 0.1675, 0.1668, 0.1670, 0.1645],\n",
      "        [0.1676, 0.1679, 0.1687, 0.1670, 0.1676, 0.1613]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test2 = selfattentionv2(input_dim=3, out_dim=2)\n",
    "output2,weights = test2.forward(inputs)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04acdb67",
   "metadata": {},
   "source": [
    "<b>Causal Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c57ace1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones(weights.shape))\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c412bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1670, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1653, 0.1648, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1655, 0.1652, 0.1642, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1657, 0.1656, 0.1647, 0.1662, 0.0000, 0.0000],\n",
      "        [0.1670, 0.1672, 0.1675, 0.1668, 0.1670, 0.0000],\n",
      "        [0.1676, 0.1679, 0.1687, 0.1670, 0.1676, 0.1613]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_weights = weights * mask\n",
    "print(masked_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d05f8f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5007, 0.4993, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3344, 0.3338, 0.3318, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2502, 0.2501, 0.2488, 0.2509, 0.0000, 0.0000],\n",
      "        [0.1999, 0.2001, 0.2004, 0.1996, 0.1999, 0.0000],\n",
      "        [0.1676, 0.1679, 0.1687, 0.1670, 0.1676, 0.1613]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_weights.sum(dim=-1, keepdim=True)\n",
    "normalized_masked_weights = masked_weights / row_sums \n",
    "print(normalized_masked_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53314e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1670,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1653, 0.1648,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1655, 0.1652, 0.1642,   -inf,   -inf,   -inf],\n",
      "        [0.1657, 0.1656, 0.1647, 0.1662,   -inf,   -inf],\n",
      "        [0.1670, 0.1672, 0.1675, 0.1668, 0.1670,   -inf],\n",
      "        [0.1676, 0.1679, 0.1687, 0.1670, 0.1676, 0.1613]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(weights.shape), diagonal=1)\n",
    "masked=weights.masked_fill(mask.bool(), float('-inf'))\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1dbba17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5001, 0.4999, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3335, 0.3334, 0.3332, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2499, 0.2501, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2001, 0.2000, 0.2000, 0.0000],\n",
      "        [0.1668, 0.1668, 0.1669, 0.1667, 0.1668, 0.1660]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "weights = torch.softmax(masked/keys2.shape[-1]**0.5, dim=1)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8d5889",
   "metadata": {},
   "source": [
    "<b>Attention weights with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "afb4aa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9998, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6663, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3336, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(p=0.5)\n",
    "dropped_weights = dropout(weights)\n",
    "print(dropped_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6398f6c4",
   "metadata": {},
   "source": [
    "<b>Causal Attention Mechanism Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b451b355",
   "metadata": {},
   "outputs": [],
   "source": [
    "class causalattention(torch.nn.Module):\n",
    "    def __init__(self, input_dim, out_dim,context_len,dropout,qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.w_query = torch.nn.Linear(input_dim, out_dim,bias=qkv_bias)\n",
    "        self.w_key = torch.nn.Linear(input_dim, out_dim,bias=qkv_bias)\n",
    "        self.w_value = torch.nn.Linear(input_dim, out_dim,bias=qkv_bias)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_len, context_len), diagonal=1))\n",
    "    def forward(self, x):\n",
    "        b,num_tokens,input_dim = x.shape\n",
    "        queries = self.w_query(x)\n",
    "        keys = self.w_key(x)\n",
    "        values = self.w_value(x)\n",
    "\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(1,2))\n",
    "        attention_scores=attention_scores.masked_fill(self.mask.bool()[:num_tokens,:num_tokens], float('-inf')) \n",
    "        attention_weights = torch.softmax(attention_scores/(keys.shape[1]**0.5), dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        output = torch.matmul(attention_weights, values)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83ee3011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "tensor([[[ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0055,  0.2110],\n",
      "         [-0.0492,  0.1841],\n",
      "         [-0.0018,  0.0584],\n",
      "         [-0.0016,  0.0483]],\n",
      "\n",
      "        [[-0.0294, -0.0246],\n",
      "         [ 0.0254,  0.1736],\n",
      "         [ 0.0168,  0.1152],\n",
      "         [ 0.0125,  0.0858],\n",
      "         [-0.0437,  0.0797],\n",
      "         [-0.0355, -0.0025]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs,inputs),dim=0)  # Create a batch of 2 identical sequences\n",
    "print(batch.shape)\n",
    "test3 = causalattention(input_dim=3, out_dim=2,context_len=6,dropout=0.5)\n",
    "contexts_vecs = test3.forward(batch)\n",
    "print(contexts_vecs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f13f15",
   "metadata": {},
   "source": [
    "<b> Multi Head attention mechanism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4124f720",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiheadattentionv1(torch.nn.Module):\n",
    "    def __init__(self, input_dim, out_dim, num_heads, context_len, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_heads = torch.nn.ModuleList([\n",
    "            causalattention(input_dim, out_dim, context_len, dropout, qkv_bias) \n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.attention_heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33734ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0155, -0.0989, -0.0257,  0.0369],\n",
      "         [-0.0426, -0.0455, -0.0035, -0.0393],\n",
      "         [ 0.0407, -0.0665, -0.0009, -0.0094],\n",
      "         [ 0.0250, -0.1343, -0.0020, -0.0236],\n",
      "         [ 0.0415, -0.0551,  0.0331, -0.0256]],\n",
      "\n",
      "        [[ 0.1580, -0.0613,  0.0000,  0.0000],\n",
      "         [-0.0637, -0.0681, -0.0257,  0.0369],\n",
      "         [ 0.0119, -0.1339, -0.0183,  0.0120],\n",
      "         [-0.0095, -0.1013, -0.0035, -0.0390],\n",
      "         [ 0.0250, -0.1343, -0.0102,  0.0147],\n",
      "         [ 0.0049, -0.0333,  0.0029,  0.0358]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test4 = Multiheadattentionv1(input_dim=3, out_dim=2, num_heads=2, context_len=6, dropout=0.5)\n",
    "contexts_vecs_multihead = test4.forward(batch)\n",
    "print(contexts_vecs_multihead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeac7af",
   "metadata": {},
   "source": [
    "<b>Multi head attention with weight splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db2b4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiheadattentionv2(torch.nn.Module):\n",
    "    def __init__(self, input_dim, out_dim, num_heads, context_len, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert out_dim % num_heads == 0, \"out_dim must be divisible by num_heads\"\n",
    "        self.out_dim = out_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = out_dim // num_heads\n",
    "        self.w_query = torch.nn.Linear(input_dim, out_dim, bias=qkv_bias)\n",
    "        self.w_key = torch.nn.Linear(input_dim, out_dim, bias=qkv_bias)\n",
    "        self.w_value = torch.nn.Linear(input_dim, out_dim, bias=qkv_bias)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_len, context_len), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, input_dim = x.shape\n",
    "        \n",
    "        # Split and reshape for multi-head attention\n",
    "        queries = self.w_query(x).view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        keys = self.w_key(x).view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        values = self.w_value(x).view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        \n",
    "        attention_scores = torch.matmul(queries, keys.transpose(2, 3))\n",
    "        attention_scores = attention_scores.masked_fill(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attention_weights = torch.softmax(attention_scores/keys.shape[-1]**0.5, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        output = torch.matmul(attention_weights, values).transpose(1, 2).contiguous().view(b, num_tokens, self.out_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10217611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0089,  0.0039,  0.0851,  0.1502, -0.1550, -0.1186],\n",
      "        [ 0.1058, -0.0568, -0.0158,  0.0961, -0.1254, -0.0656],\n",
      "        [-0.1381, -0.1575,  0.1219,  0.0845,  0.1126,  0.0127],\n",
      "        [-0.0756,  0.1092, -0.0810, -0.0303,  0.0479,  0.0165],\n",
      "        [-0.0089,  0.0039,  0.0851,  0.1502, -0.1550, -0.1186],\n",
      "        [ 0.1076,  0.1495, -0.0836, -0.0627,  0.1230, -0.0256]])\n",
      "torch.Size([2, 6, 6])\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0037, -0.0019,  0.0136, -0.0799,  0.0117, -0.0673],\n",
      "         [-0.0152,  0.0084,  0.0005, -0.0534,  0.0078, -0.0450],\n",
      "         [-0.0286, -0.0125, -0.0564, -0.0629,  0.0240, -0.0547],\n",
      "         [-0.0134, -0.0146, -0.0363, -0.0235,  0.0160,  0.0019],\n",
      "         [-0.0097, -0.0127, -0.0251, -0.0686,  0.0199, -0.0589]],\n",
      "\n",
      "        [[ 0.0074, -0.0038,  0.0272,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000, -0.0799,  0.0117, -0.0673],\n",
      "         [-0.0152,  0.0084,  0.0005,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0036, -0.0506, -0.0404],\n",
      "         [-0.0109, -0.0167, -0.0351, -0.0207, -0.0243, -0.0304],\n",
      "         [-0.0010,  0.0139,  0.0250, -0.0856,  0.0307, -0.0236]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "words=[\"the\",\"sun\",\"rises\",\"in\",\"the\",\"east\"]\n",
    "model=Word2Vec([words],min_count=1,vector_size=6)\n",
    "inputs = torch.tensor(model.wv[words])  # Convert words to their corresponding vectors\n",
    "print(inputs)\n",
    "batch = torch.stack((inputs,inputs),dim=0)  # Create a batch of 2 identical sequences\n",
    "print(batch.shape)\n",
    "test4 = Multiheadattentionv2(input_dim=6, out_dim=6, num_heads=2, context_len=6, dropout=0.5)\n",
    "contexts_vecs_multihead = test4.forward(batch)\n",
    "print(contexts_vecs_multihead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5527ddc",
   "metadata": {},
   "source": [
    "<b>Implementing Dummy GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "094881fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M={\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_len\": 1024,\n",
    "    \"n_embd\": 768,\n",
    "    \"n_layer\": 12,\n",
    "    \"n_head\": 12,\n",
    "    \"dropout\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "461faeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class DummyGPTModel(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embedding = torch.nn.Embedding(config['vocab_size'], config['n_embd'])\n",
    "        self.position_embedding = torch.nn.Embedding(config['context_len'], config['n_embd'])\n",
    "        self.dropout = torch.nn.Dropout(config['dropout'])\n",
    "        \n",
    "        #Placeholder for the actual transformer blocks\n",
    "        self.trf = torch.nn.Sequential(\n",
    "            *[DummyTransformer(config) for _ in range(config['n_layer'])])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.final_norm = DummyLayerNorm(config['n_embd'])\n",
    "        self.head = torch.nn.Linear(config['n_embd'], config['vocab_size'], bias=False)\n",
    "        \n",
    "\n",
    "    def forward(self, idx):\n",
    "        b, t = idx.shape\n",
    "        token_embeddings = self.token_embedding(idx)\n",
    "        position_embeddings = self.position_embedding(torch.arange(t, device=idx.device))\n",
    "        \n",
    "        x = token_embeddings + position_embeddings\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.trf(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "class DummyTransformer(torch.nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "    def forward(self,x):\n",
    "        return x\n",
    "class DummyLayerNorm(torch.nn.Module):\n",
    "    def __init__(self,shape,eps=1e-5):\n",
    "        super().__init__()\n",
    "    def forward(self,x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c49ad57",
   "metadata": {},
   "source": [
    "<b> Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f558296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  464,  4252, 16736,   287,   262],\n",
      "        [  464,  4252,  5621,   287,   262]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch=[]\n",
    "txt1 = \"The sun rises in the\"\n",
    "txt2 = \"The sun sets in the\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbce10a",
   "metadata": {},
   "source": [
    "<b>Instance of DummyGPTModel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50a92944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 50257])\n",
      "tensor([[[ 0.5209,  0.5520, -0.1626,  ..., -0.6756,  0.4891,  1.0015],\n",
      "         [ 0.2441,  1.2358,  0.7340,  ...,  0.8618,  0.6503,  0.6837],\n",
      "         [-0.1495,  0.3894, -0.2040,  ...,  0.3658, -0.6975,  0.0046],\n",
      "         [-0.4361,  2.2998, -0.8146,  ...,  1.0815, -0.0610,  0.4576],\n",
      "         [-0.6581, -0.4606,  0.6273,  ...,  0.6359, -1.1219, -0.1483]],\n",
      "\n",
      "        [[ 0.6502,  0.6544, -0.4624,  ..., -0.6154,  0.2181,  1.1435],\n",
      "         [ 0.4852,  1.2206, -0.0328,  ...,  0.7643,  0.6751,  0.7432],\n",
      "         [ 0.4855,  0.2990, -0.0415,  ..., -0.3032,  0.9354,  0.3412],\n",
      "         [-0.1921,  1.6800, -0.7408,  ...,  0.7503,  0.0276,  0.4491],\n",
      "         [-0.4396, -0.8658,  0.2206,  ...,  0.7252, -1.3121,  0.0530]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e93ddef",
   "metadata": {},
   "source": [
    "<b> Layer Normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5ce29af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(torch.nn.Module):\n",
    "    def __init__(self, n_embd, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(n_embd))\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(n_embd))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        variance = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        x_normalized = (x - mean) / torch.sqrt(variance + self.eps)\n",
    "        return self.gamma * x_normalized + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "657cb643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.1467e-08],\n",
      "        [6.9539e-08]], grad_fn=<MeanBackward1>)\n",
      "tensor([[1.1999],\n",
      "        [1.1999]], grad_fn=<VarBackward0>)\n",
      "tensor([[ 0.3324,  0.8349, -0.2272,  0.0529, -2.0204,  1.0276],\n",
      "        [-1.4542, -0.9964,  1.2767,  0.3657,  1.0374, -0.2291]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch = torch.randn(2, 6)\n",
    "ln = LayerNorm(n_embd=6)\n",
    "output = ln.forward(batch)\n",
    "mean = output.mean(dim=-1,keepdim=True)\n",
    "var = output.var(dim=-1,keepdim=True)\n",
    "print(mean)\n",
    "print(var)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312657d3",
   "metadata": {},
   "source": [
    "<b> GELU Activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df3b1807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class GELU(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi)) * (x + 0.044715 * (x ** 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "19201091",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(cfg['n_embd'], cfg['n_embd']*4),\n",
    "            GELU(),\n",
    "            torch.nn.Linear(cfg['n_embd']*4, cfg['n_embd']),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4435666e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "batch = torch.randn(2, 6, 768)\n",
    "output = ffn.forward(batch)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd581f37",
   "metadata": {},
   "source": [
    "<b>Transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "609c70b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_block(torch.nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(cfg['n_embd'])\n",
    "        self.attn = Multiheadattentionv2(cfg['n_embd'], cfg['n_embd'], cfg['n_head'], cfg['context_len'], cfg['dropout'], cfg['qkv_bias'])\n",
    "        self.ln2 = LayerNorm(cfg['n_embd'])\n",
    "        self.ffn = FeedForward(cfg)\n",
    "        self.dropout = torch.nn.Dropout(cfg['dropout'])\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.ln1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "        \n",
    "        shortcut = x\n",
    "        x = self.ln2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "169e991e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 768])\n",
      "torch.Size([2, 6, 768])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(2, 6, 768)\n",
    "block = transformer_block(GPT_CONFIG_124M)\n",
    "output = block.forward(input)\n",
    "print(input.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de947433",
   "metadata": {},
   "source": [
    "<b> Implementing GPT from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6eac9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M={\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_len\": 256,\n",
    "    \"n_embd\": 768,\n",
    "    \"n_layer\": 12,\n",
    "    \"n_head\": 12,\n",
    "    \"dropout\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "15369aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embedding = torch.nn.Embedding(config['vocab_size'], config['n_embd'])\n",
    "        self.position_embedding = torch.nn.Embedding(config['context_len'], config['n_embd'])\n",
    "        self.dropout = torch.nn.Dropout(config['dropout'])\n",
    "        \n",
    "        self.trf = torch.nn.Sequential(\n",
    "            *[transformer_block(config) for _ in range(config['n_layer'])])\n",
    "\n",
    "        self.final_norm = LayerNorm(config['n_embd'])\n",
    "        self.head = torch.nn.Linear(config['n_embd'], config['vocab_size'], bias=False)\n",
    "        \n",
    "\n",
    "    def forward(self, idx):\n",
    "        b, t = idx.shape\n",
    "        token_embeddings = self.token_embedding(idx)\n",
    "        position_embeddings = self.position_embedding(torch.arange(t, device=idx.device))\n",
    "        \n",
    "        x = token_embeddings + position_embeddings\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.trf(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4f0a6c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "tensor([[  464,  4252, 16736,   287],\n",
      "        [  464,  4252,  5621,   287]])\n",
      "torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.7702, -0.4592,  0.3900,  ..., -0.3690,  0.0256, -0.0773],\n",
      "         [ 0.1550,  0.8860,  0.4371,  ..., -0.0447, -1.0403,  0.0203],\n",
      "         [ 0.6085,  0.6617, -0.0735,  ..., -0.8982,  0.4881,  0.0329],\n",
      "         [ 0.1109, -0.2661,  0.0500,  ...,  0.3325, -0.4801, -0.1991]],\n",
      "\n",
      "        [[ 0.7637, -0.0254,  0.3722,  ..., -0.5632,  0.0634,  0.0950],\n",
      "         [ 0.2272,  0.7894,  0.6884,  ...,  0.1864, -0.4143,  0.6640],\n",
      "         [-0.0578,  0.2830, -0.0868,  ..., -0.0354, -0.2316,  0.6870],\n",
      "         [-0.1122, -0.4252, -0.5834,  ...,  0.7350, -0.4191,  0.0331]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch=[]\n",
    "txt1 = \"The sun rises in\"\n",
    "txt2 = \"The sun sets in\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "print(batch.shape)\n",
    "print(batch)\n",
    "output = model(batch)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a43533b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 155,332,608\n"
     ]
    }
   ],
   "source": [
    "total_param = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_param:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7033bf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters without the head: 116,735,232\n"
     ]
    }
   ],
   "source": [
    "total_param_2=total_param-sum(p.numel() for p in model.head.parameters())\n",
    "print(f\"Total parameters without the head: {total_param_2:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5052e622",
   "metadata": {},
   "source": [
    "<b> Generating text from output tokens:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab6fb80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output_txt(model,input, max_new_tokens,context_len):\n",
    "    for i in range(max_new_tokens):\n",
    "        input_cond = input[:,-context_len:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_cond)\n",
    "        logits = logits[:,-1,:]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.argmax(probs,dim=-1,keepdim=True)\n",
    "        input = torch.cat((input, next_token), dim=1)\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "836d344d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  464,  4252, 16736,   287]])\n"
     ]
    }
   ],
   "source": [
    "input = \"The sun rises in\"\n",
    "encoded_input = torch.tensor(tokenizer.encode(input)).unsqueeze(0) \n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "344e356f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  464,  4252, 16736,   287, 38783, 12171, 47776, 29697, 43110, 24844]])\n",
      "The sun rises in483ulkimeoAAA sparing ===\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "output_tokens = generate_output_txt(model, encoded_input, max_new_tokens=6, context_len=GPT_CONFIG_124M['context_len'])\n",
    "print(output_tokens)\n",
    "print(tokenizer.decode(output_tokens[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4ecab6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun rises in483ulkimeoAAA sparing ===\n"
     ]
    }
   ],
   "source": [
    "def text_to_tokens(text,tokenizer):\n",
    "    return torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
    "def tokens_to_text(tokens,tokenizer):\n",
    "    return tokenizer.decode((tokens.squeeze(0)).tolist())\n",
    "\n",
    "ex = \"The sun rises in\"\n",
    "toeken_ids=generate_output_txt(model,text_to_tokens(ex,tokenizer), max_new_tokens=6,context_len=GPT_CONFIG_124M['context_len'])\n",
    "print(tokens_to_text(toeken_ids,tokenizer))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5ecc08e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[16833,  3626,  6100],\n",
      "        [   40,  1107,   588]])\n",
      "tensor([[ 3626,  6100,   345],\n",
      "        [ 1107,   588, 11311]])\n"
     ]
    }
   ],
   "source": [
    "inputs = text_to_tokens(\"every effort moves\",tokenizer)\n",
    "inputs = torch.cat([inputs, text_to_tokens(\"I really like\",tokenizer)])\n",
    "print(inputs)\n",
    "target = torch.tensor([[3626,6100,345],\n",
    "                       [1107,588,11311]])\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b59df84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[17891, 31803, 43889],\n",
      "        [40728, 17646, 47633]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probabs = torch.softmax(logits, dim=-1)\n",
    "output = torch.argmax(probabs,dim=-1)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1b5dbca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:  effort moves you\n",
      "Output:  Banglmapsï¿½\n"
     ]
    }
   ],
   "source": [
    "print(\"Target:\",tokens_to_text(target[0],tokenizer))\n",
    "print(\"Output:\",tokens_to_text(output[0],tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4372b000",
   "metadata": {},
   "source": [
    "<b> Cross Entropy Loss :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "69596632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2980e-05, 1.5123e-05, 3.1214e-05])\n",
      "tensor([1.1114e-05, 1.9711e-05, 3.5344e-05])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0 \n",
    "target_1 = probabs[text_idx,[0,1,2],target[text_idx]]\n",
    "print(target_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_2 = probabs[text_idx,[0,1,2],target[text_idx]]\n",
    "print(target_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "57840d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-11.2521, -11.0993, -10.3747, -11.4073, -10.8343, -10.2504])\n"
     ]
    }
   ],
   "source": [
    "#Log of all token probability\n",
    "log_probas  = torch.log(torch.cat((target_1,target_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e4993733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.8697)\n"
     ]
    }
   ],
   "source": [
    "#Calculate average probability\n",
    "avg_log_prob = torch.mean(log_probas)\n",
    "print(avg_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0948ec01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8697)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_prob = -avg_log_prob\n",
    "print(neg_avg_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8466c92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 50257]) torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "#Using PyTorch Cross Entropy Loss\n",
    "logits_flat = logits.flatten(0,1)\n",
    "target = target.flatten()\n",
    "print(logits_flat.shape,target.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b831eaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8697)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aa6b23b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b7062177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c5989d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20479\n",
      "5145\n"
     ]
    }
   ],
   "source": [
    "total_chars = len(text)\n",
    "total_tokens = len(tokenizer.encode(text))\n",
    "\n",
    "print(total_chars)\n",
    "print(total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "be05740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, context_len,stride):\n",
    "        self.inputs = []\n",
    "        self.targets =[]\n",
    "        \n",
    "        tokens = tokenizer.encode(text)\n",
    "        \n",
    "        for i in range(0, len(tokens) - context_len, stride):\n",
    "            input_ids = tokens[i:i + context_len]\n",
    "            target_ids = tokens[i + 1:i + context_len + 1]\n",
    "            self.inputs.append(torch.tensor(input_ids))\n",
    "            self.targets.append(torch.tensor(target_ids))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "    \n",
    "def create_dataloader(text,batch_size=4,context_len =256,stride=128,shuffle=True,drop_last=True,num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDataset(text, tokenizer, context_len,stride)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last,num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6b05f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "train_size = int(train_ratio * len(text))\n",
    "train_text = text[:train_size]\n",
    "val_text = text[train_size:]\n",
    "\n",
    "train_dataloader = create_dataloader(train_text,batch_size=2,context_len =GPT_CONFIG_124M[\"context_len\"],stride=GPT_CONFIG_124M[\"context_len\"],shuffle=True,drop_last=True,num_workers=0)\n",
    "val_dataloader = create_dataloader(val_text,batch_size=2,context_len =GPT_CONFIG_124M[\"context_len\"],stride=GPT_CONFIG_124M[\"context_len\"],shuffle=False,drop_last=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ae5c81ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))\n",
    "for xb,yb in train_dataloader:\n",
    "    print(xb.shape,yb.shape)\n",
    "\n",
    "\n",
    "for xb,yb in val_dataloader:\n",
    "    print(xb.shape,yb.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e2da3df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(input_batch,output_batch,model,device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    output_batch = output_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0,1),output_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(dataloader,model,device,num_batches=None):\n",
    "    loss = 0\n",
    "    if len(dataloader)==0:\n",
    "        return 'nan'\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(dataloader)\n",
    "    else:\n",
    "        num_batches = min(num_batches,len(dataloader))\n",
    "    for i,(xb,yb) in enumerate(dataloader):\n",
    "        if i>=num_batches:\n",
    "            break\n",
    "        loss += calc_loss(xb,yb,model,device).item()\n",
    "    return loss/num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "97e5797d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 10.9952, Val loss: 10.9774\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_dataloader,model,device,num_batches=10)\n",
    "    val_loss = calc_loss_loader(val_dataloader,model,device,num_batches=10)\n",
    "print(f\"Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5e77fc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simple(model,train_dataloader,val_dataloader,optimizer,device,epochs=3,eval_freq=200,eval_iter=10,start_context=0,tokenizer=None):\n",
    "    train_losses,val_losses,track_tokens_seen=[],[],[]\n",
    "    tokens_seen,global_step=0,-1\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for i,(xb,yb) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss(xb,yb,model,device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += xb.numel()\n",
    "            global_step += 1\n",
    "            if (global_step) % eval_freq == 0:\n",
    "                train_loss,val_loss = eval_simple(model,train_dataloader,val_dataloader,device,eval_batches=eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Epoch {epoch+1}, Step {i+1}, Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}\")\n",
    "                model.train()\n",
    "        \n",
    "        generate_text(model,tokenizer,device,start_context)\n",
    "    \n",
    "    return train_losses,val_losses,track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "715ed524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_simple(model,train_dataloader,val_dataloader,device,eval_batches=10):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_dataloader,model,device,num_batches=eval_batches)\n",
    "        val_loss = calc_loss_loader(val_dataloader,model,device,num_batches=eval_batches)\n",
    "    return train_loss,val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6eec8893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model,tokenizer,device, start_text):\n",
    "    model.eval()\n",
    "    context = model.position_embedding.weight.shape[0]\n",
    "    encoded_input = text_to_tokens(start_text,tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        output_tokens = generate_output_txt(model, encoded_input, max_new_tokens=50, context_len=context)\n",
    "    decoded_text = tokens_to_text(output_tokens[0],tokenizer)\n",
    "    print(decoded_text.replace('\\n',' '))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "83454d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 1, Train loss: 9.9751, Val loss: 10.0935\n",
      "Epoch 1, Step 6, Train loss: 8.2315, Val loss: 8.3875\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Epoch 2, Step 2, Train loss: 6.7807, Val loss: 7.0953\n",
      "Epoch 2, Step 7, Train loss: 6.0749, Val loss: 6.6020\n",
      "Every effort moves you, the, the, the, the, the, the, the, the the the, the, the, the, the the, the, the, the, the, the the, the, the, the the, the, the,\n",
      "Epoch 3, Step 3, Train loss: 5.7674, Val loss: 6.4992\n",
      "Epoch 3, Step 8, Train loss: 5.3706, Val loss: 6.3529\n",
      "Every effort moves you, I had to the I had. Gisburn--I. Gisburn----I, and I had I had to the I had to the I had to the. I had I had been, and--I--I was,\n",
      "Epoch 4, Step 4, Train loss: 5.1009, Val loss: 6.3525\n",
      "Epoch 4, Step 9, Train loss: 4.9897, Val loss: 6.3703\n",
      "Every effort moves you.                                                 \n",
      "Epoch 5, Step 5, Train loss: 4.3066, Val loss: 6.3273\n",
      "Every effort moves you.            \"I and he was, and he was the.                   \"--as he had been.\n",
      "Epoch 6, Step 1, Train loss: 4.0411, Val loss: 6.1964\n",
      "Epoch 6, Step 6, Train loss: 3.7509, Val loss: 6.2307\n",
      "Every effort moves you know the donkey, and I felt of the fact, and in the last of the fact--and, and in the fact--and to the fact, and I had been, and in the fact, and Mrs.      \n",
      "Epoch 7, Step 2, Train loss: 3.1323, Val loss: 6.2260\n",
      "Epoch 7, Step 7, Train loss: 2.8526, Val loss: 6.2442\n",
      "Every effort moves you of the donkey, and I felt, and--I had the fact--the not to my work, and in the fact--I had been.       \"--the, the donkey, and in a little. I\n",
      "Epoch 8, Step 3, Train loss: 2.4390, Val loss: 6.2311\n",
      "Epoch 8, Step 8, Train loss: 2.0035, Val loss: 6.2780\n",
      "Every effort moves you?\"            \"I turned, and he had I had the fact, and I had a smile; and I had to the donkey, and I had the donkey, and I had a at the\n",
      "Epoch 9, Step 4, Train loss: 1.5484, Val loss: 6.3314\n",
      "Epoch 9, Step 9, Train loss: 1.3815, Val loss: 6.3241\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with a laugh: \"Yes--and by me!\"              \"--as I had been and were, and he was.\n",
      "Epoch 10, Step 5, Train loss: 1.1163, Val loss: 6.3831\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"          \"Oh, my elbow and as his pictures--as you know. I was.\n",
      "Training completed in 3.88 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004,weight_decay=1e-1)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "train_loss,val_loss,tokens_seen = train_simple(model,train_dataloader,val_dataloader,optimizer,device,epochs=num_epochs,eval_freq=5,eval_iter=5,start_context=\"Every effort moves you\",tokenizer=tokenizer)\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Training completed in {execution_time/60:.2f} minutes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep)",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
